"""
GPT-5 Benchmark æµ‹è¯•è„šæœ¬
è¿æ¥ MCP æœåŠ¡å™¨ï¼Œé€šè¿‡ MCP åè®®è¿›è¡Œå·¥å…·è°ƒç”¨
"""

import json
import os
import sys
import base64
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from openai import OpenAI

# MCP å®¢æˆ·ç«¯å¯¼å…¥
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

sys.path.insert(0, str(Path(__file__).parent))

from core.vega_service import get_vega_service


# =============================================================================
# é…ç½®
# =============================================================================

GPT_CONFIG = {
    'api_key_env': 'OPENAI_API_KEY',
    'base_url': 'https://api.oaipro.com/v1',
    'model': 'gpt-5',
    'max_iterations': 8,
    'temperature': 0,
    'timeout': 180,
    'save_images': True,
    'max_tokens' : 2000,
}

MCP_SERVER_PATH = Path(__file__).parent / 'chart_tools_mcp_server.py'


# =============================================================================
# MCP å®¢æˆ·ç«¯è¾…åŠ©å‡½æ•°
# =============================================================================

def _fix_schema_types(schema: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä¿®å¤ JSON Schema ä¸­å¸¸è§çš„ä¸å®Œæ•´å®šä¹‰ï¼Œä»¥é€šè¿‡ OpenAI/Qwen/Gemini ç­‰æ ¡éªŒã€‚
    - array: å¿…é¡»åŒ…å« items
    - object: å»ºè®®æ·»åŠ  additionalProperties
    - å»é™¤ä¸æ”¯æŒå­—æ®µï¼š$ref / nullable
    """
    if not isinstance(schema, dict):
        return schema
    
    # ç§»é™¤ä¸æ”¯æŒå­—æ®µ
    schema.pop("$ref", None)
    schema.pop("nullable", None)
    
    schema_type = schema.get("type")
    
    # å¤„ç† object
    if schema_type == "object":
        props = schema.get("properties", {})
        for prop_name, prop_def in props.items():
            props[prop_name] = _fix_schema_types_with_name(prop_def, prop_name)
        schema["properties"] = props
        
        # å…è®¸åŠ¨æ€é”®ï¼Œé¿å…ä¸¥æ ¼æ ¡éªŒæŠ¥é”™
        if "additionalProperties" not in schema:
            schema["additionalProperties"] = True
    
    # å¤„ç† array
    if schema_type == "array":
        if "items" not in schema:
            schema["items"] = {"type": "string"}
        else:
            schema["items"] = _fix_schema_types(schema["items"])
    
    return schema


def _fix_schema_types_with_name(prop_def: Any, prop_name: str) -> Any:
    """
    æ ¹æ®å­—æ®µåå¯¹ array çš„ items åšåˆç†é»˜è®¤æ¨æ–­ã€‚
    """
    if not isinstance(prop_def, dict):
        return prop_def
    
    prop_type = prop_def.get("type")
    
    # æ•°ç»„é¡¹æ¨æ–­
    if prop_type == "array" and "items" not in prop_def:
        name_lower = prop_name.lower()
        if any(k in name_lower for k in ["range", "position", "coord", "point", "area", "bbox"]):
            prop_def["items"] = {"type": "number"}
        elif any(k in name_lower for k in ["id", "name", "label", "category", "field"]):
            prop_def["items"] = {"type": "string"}
        else:
            prop_def["items"] = {"type": "string"}
    
    # é€’å½’ä¿®å¤å­ schema
    return _fix_schema_types(prop_def)


def convert_mcp_tools_to_openai_format(mcp_tools) -> List[Dict[str, Any]]:
    """
    å°† MCP å·¥å…·å®šä¹‰è½¬æ¢ä¸º OpenAI Function Calling æ ¼å¼ï¼Œå¹¶åš Schema æ ‡å‡†åŒ–ã€‚
    """
    openai_tools = []
    
    for tool in mcp_tools:
        parameters = tool.inputSchema if tool.inputSchema else {
            "type": "object",
            "properties": {},
            "required": []
        }
        
        # ä»å‚æ•°ä¸­ç§»é™¤ vega_specï¼ˆæ¨¡å‹ä¸éœ€è¦çŸ¥é“è¿™ä¸ªå‚æ•°ï¼‰ï¼Œå¹¶ä¿®å¤ Schema
        params = _fix_schema_types(parameters)
        if "properties" in params and "vega_spec" in params["properties"]:
            del params["properties"]["vega_spec"]
        if "required" in params and "vega_spec" in params["required"]:
            params["required"].remove("vega_spec")
        
        openai_tool = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description or "",
                "parameters": params
            }
        }
        
        openai_tools.append(openai_tool)
    
    return openai_tools


# =============================================================================
# ç³»ç»Ÿæç¤ºè¯
# =============================================================================

def get_system_prompt(chart_type: str) -> str:
    """Generate system prompt for the given chart type."""
    
    return f"""You are a professional data visualization analysis assistant. Your task is to analyze chart data based on user questions and discover valuable insights.

Current chart type: **{chart_type}**

## Analysis Strategy
1. Carefully read the user question and understand the task
2. Use the provided tools for analysis (if a specific tool is mentioned in the question, you must use that tool)
3. Answer the user question based on tool results

## Tool Selection Guidelines
- **scatter_plot**: select_region, calculate_correlation, identify_clusters, zoom_dense_area, brush_region, change_encoding,filter_categorical,show_regression,
- **bar_chart**: sort_bars, highlight_top_n,filter_categories,expand_stack,toggle_stack_mode,add_bars,remove_bars,add_bar_items,remove_bar_items,change_encoding
- **line_chart**: zoom_time_range, detect_anomalies,highlight_trend,bold_lines,filter_lines,show_moving_average,focus_lines,drilldown_line_time,reset_line_drilldown,resample_time,reset_resample,change_encoding
- **heatmap**: adjust_color_scale,filter_cells,highlight_region,cluster_rows_cols,select_submatrix,find_extremes,threshold_mask,drilldown_time,reset_drilldown,add_marginal_bars,transpose,change_encoding
- **parallel_coordinates**: filter_dimension, highlight_cluster, reorder_dimensions,filter_by_category,highlight_category,hide_dimensions,reset_hidden_dimensions
- **sankey_diagram**: trace_node, highlight_path, filter_flow,calculate_conversion_rate,collapse_nodes,expand_node,auto_collapse_by_rank,color_flows,find_bottleneck,reorder_nodes_in_layer

## Answer Format Specification
Numeric Questions
When to use: Questions asking "how many", "what is the value", "count", "coefficient", "percentage"
Format: Single number only
Example: "How many cars are there in the dataset?" -> "100"

Categorical Questions
When to use: Questions asking "which", "what category", "what type", "which country/region"
Format: Single word/phrase only
Example: "Which country has the highest horsepower?" -> "United States"


Boolean Questions
When to use: Questions asking "is there", "does it", "are they", "yes/no question"
Format: `Yes` or `No` only
Example: "Are there any cars with horsepower greater than 200?" -> "Yes"

Open-ended Questions
When to use: Questions asking about vague exploration of the data
Format: freely answer the question with sentences
Example: "Reveal subtle differences in temperature patterns across cities and months
" -> "Denverâ€™s June temperature (around 22Â°C) is now visibly higher than its January temperature (around 8Â°C).\nMiamiâ€™s temperatures are consistently high across all months, with its lowest monthly temperature still being warmer than the highest temperatures in Denver or Seattle."

## Output Requirements
- After completing tool calls, provide a clear answer
- If the question requires a specific tool, ensure that tool is called
- Answers should be direct and concise"""




def get_analysis_prompt() -> str:
    """Generate analysis phase prompt."""
    return """Based on the current view and tool results, please return your analysis in JSON format:

```json
{
  "key_insights": ["Insight 1", "Insight 2"],
  "reasoning": "Your reasoning process",
  "answer": "Direct answer to the user question",
  "exploration_complete": true
}
```

Field descriptions:
- **key_insights**: Discovered insights
- **reasoning**: Reasoning process
- **answer**: Direct answer to the user question (required)
- **exploration_complete**: Whether exploration is complete (usually true)

Please ensure you return valid JSON."""

# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def load_vega_spec(vega_spec_path: str) -> dict:
    """åŠ è½½ Vega è§„èŒƒæ–‡ä»¶"""
    if not os.path.isabs(vega_spec_path):
        script_dir = Path(__file__).parent
        vega_spec_path = script_dir / vega_spec_path
    
    with open(vega_spec_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_openai_client() -> OpenAI:
    """åˆ›å»º OpenAI å®¢æˆ·ç«¯"""
    api_key = os.getenv(GPT_CONFIG['api_key_env'])
    if not api_key:
        raise ValueError(f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡: {GPT_CONFIG['api_key_env']}")
    
    return OpenAI(
        api_key=api_key,
        base_url=GPT_CONFIG['base_url'],
        timeout=GPT_CONFIG['timeout']
    )


def format_user_message_with_image(text: str, image_base64: str) -> dict:
    """æ ¼å¼åŒ–å¸¦å›¾åƒçš„ç”¨æˆ·æ¶ˆæ¯"""
    return {
        'role': 'user',
        'content': [
            {'type': 'text', 'text': text},
            {
                'type': 'image_url',
                'image_url': {
                    'url': f'data:image/png;base64,{image_base64}',
                    'detail': 'high'
                }
            }
        ]
    }


def save_image(image_base64: str, output_path: Path) -> None:
    """ä¿å­˜ base64 å›¾åƒåˆ°æ–‡ä»¶"""
    image_data = base64.b64decode(image_base64)
    with open(output_path, 'wb') as f:
        f.write(image_data)


def parse_json_from_response(content: str) -> dict:
    """ä» GPT å“åº”ä¸­è§£æ JSON"""
    if not content:
        return {}
    
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass
    
    # å°è¯•æå– ```json ... ``` å—
    if "```json" in content:
        try:
            json_str = content.split("```json")[1].split("```")[0].strip()
            return json.loads(json_str)
        except (IndexError, json.JSONDecodeError):
            pass
    
    # å°è¯•æå– ``` ... ``` å—
    if "```" in content:
        try:
            json_str = content.split("```")[1].split("```")[0].strip()
            return json.loads(json_str)
        except (IndexError, json.JSONDecodeError):
            pass
    
    return {}


# =============================================================================
# ä¸»è¦æ‰§è¡Œé€»è¾‘ï¼ˆå¼‚æ­¥ï¼‰
# =============================================================================

async def run_benchmark_async(task_path: str) -> dict:
    """è¿è¡Œ benchmark æµ‹è¯•"""
    
    # 1. åŠ è½½ä»»åŠ¡
    with open(task_path, 'r', encoding='utf-8') as f:
        task = json.load(f)
    
    task_id = task['task_id']
    vega_spec_path = task['task']['initial_visualization']['vega_spec_path']
    vega_spec = load_vega_spec(vega_spec_path)
    query = task['task']['query']
    chart_type = task['metadata'].get('chart_type', 'scatter_plot')
    
    print(f" ä»»åŠ¡: {task_id}")
    print(f" å›¾è¡¨ç±»å‹: {chart_type}")
    print(f" æŸ¥è¯¢: {query}\n")
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_name = GPT_CONFIG['model'].replace('.', '_').replace('/', '_')
    output_base_dir = Path('benchmark/results/gpt5_mcp')
    run_dir = output_base_dir / f"{task_id}_{model_name}_{timestamp}"
    images_dir = run_dir / 'images'
    
    if GPT_CONFIG['save_images']:
        images_dir.mkdir(parents=True, exist_ok=True)
    
    # 3. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å’Œ Vega æœåŠ¡
    openai_client = get_openai_client()
    vega_service = get_vega_service()
    
    # 4. æ¸²æŸ“åˆå§‹å›¾åƒ
    render_result = vega_service.render(vega_spec)
    if not render_result['success']:
        print(f" æ¸²æŸ“å¤±è´¥: {render_result.get('error')}")
        return None
    
    current_image = render_result['image_base64']
    current_spec = vega_spec
    
    if GPT_CONFIG['save_images']:
        save_image(current_image, images_dir / 'iteration_0_initial.png')
    
    # 5. è¿æ¥ MCP æœåŠ¡å™¨
    print(" è¿æ¥ MCP æœåŠ¡å™¨...")
    
    server_params = StdioServerParameters(
        command="python",
        args=[str(MCP_SERVER_PATH)]
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as mcp_session:
            # åˆå§‹åŒ– MCP ä¼šè¯
            await mcp_session.initialize()
            print(" MCP æœåŠ¡å™¨è¿æ¥æˆåŠŸ")
            
            # 6. ä» MCP è·å–å·¥å…·åˆ—è¡¨
            mcp_tools_response = await mcp_session.list_tools()
            mcp_tools = mcp_tools_response.tools
            print(f"ğŸ”§ ä» MCP è·å–åˆ° {len(mcp_tools)} ä¸ªå·¥å…·")
            
            # è½¬æ¢ä¸º OpenAI æ ¼å¼
            openai_tools = convert_mcp_tools_to_openai_format(mcp_tools)
            
            # 7. åˆå§‹åŒ–å¯¹è¯
            system_prompt = get_system_prompt(chart_type)
            messages = [
                {'role': 'system', 'content': system_prompt},
                format_user_message_with_image(f"è¯·æ¢ç´¢è¿™ä¸ªå›¾è¡¨å¹¶å‘ç°æ´å¯Ÿã€‚\n\nç”¨æˆ·æŸ¥è¯¢ï¼š{query}", current_image)
            ]
            
            explorations = []
            all_tools_called = []
            
            print(" å¼€å§‹æ¢ç´¢åˆ†æ...")
            print("=" * 70)
            
            # 8. å¤šè½®å¯¹è¯å¾ªç¯ï¼ˆä¸¤é˜¶æ®µ MCPï¼‰
            for i in range(GPT_CONFIG['max_iterations']):
                print(f"\n{'='*20} ç¬¬ {i+1} è½® {'='*20}")
                
                # ========== é˜¶æ®µ1ï¼šå·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰ ==========
                print("\n é˜¶æ®µ1ï¼šå·¥å…·è°ƒç”¨...")
                
                response1 = openai_client.chat.completions.create(
                    model=GPT_CONFIG['model'],
                    messages=messages,
                    tools=openai_tools,
                    tool_choice="auto",
                    temperature=GPT_CONFIG['temperature'],
                )
                
                message1 = response1.choices[0].message
                
                # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
                assistant_msg = {'role': 'assistant', 'content': message1.content}
                if message1.tool_calls:
                    assistant_msg['tool_calls'] = [
                        {
                            'id': tc.id,
                            'type': 'function',
                            'function': {
                                'name': tc.function.name,
                                'arguments': tc.function.arguments
                            }
                        }
                        for tc in message1.tool_calls
                    ]
                messages.append(assistant_msg)
                
                # æ„å»º exploration è®°å½•
                exploration = {
                    'iteration': i + 1,
                    'success': True,
                    'timestamp': datetime.now().isoformat(),
                    'analysis_summary': {
                        'key_insights': [],
                        'reasoning': ''
                    },
                    'tool_execution': None
                }
                
                # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆé€šè¿‡ MCPï¼‰
                if message1.tool_calls:
                    for tool_call in message1.tool_calls:
                        tool_name = tool_call.function.name
                        tool_args = json.loads(tool_call.function.arguments)
                        
                        print(f"\n é€šè¿‡ MCP è°ƒç”¨å·¥å…·: {tool_name}")
                        print(f"   å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}")
                        
                        # ========== çœŸæ­£çš„ MCP å·¥å…·è°ƒç”¨ ==========
                        # æ·»åŠ  vega_spec å‚æ•°
                        mcp_args = {**tool_args, 'vega_spec': current_spec}
                        
                        mcp_result = await mcp_session.call_tool(
                            name=tool_name,
                            arguments=mcp_args
                        )
                        
                        # è°ƒè¯•ï¼šæ‰“å° MCP è¿”å›çš„åŸå§‹å†…å®¹ï¼Œä¾¿äºç¡®è®¤ç±»å‹/ç»“æ„
                        print(f"   â†©ï¸ MCP åŸå§‹è¿”å›: {mcp_result.content}")
                        
                        # è§£æ MCP è¿”å›ç»“æœ
                        tool_result = {}
                        if mcp_result.content:
                            for content_item in mcp_result.content:
                                if content_item.type == 'text':
                                    try:
                                        tool_result = json.loads(content_item.text)
                                    except json.JSONDecodeError:
                                        tool_result = {'success': False, 'message': content_item.text}
                        
                        all_tools_called.append(tool_name)
                        
                        exploration['tool_execution'] = {
                            'tool_name': tool_name,
                            'parameters': tool_args,
                            'result': {
                                'success': tool_result.get('success', False),
                                'message': tool_result.get('message', '')
                            }
                        }
                        
                        # æ„å»ºå·¥å…·ç»“æœæ¶ˆæ¯
                        tool_response_content = json.dumps({
                            'success': tool_result.get('success', False),
                            'message': tool_result.get('message', ''),
                            'data': tool_result.get('cluster_statistics') or tool_result.get('correlation') or tool_result.get('summary') or {}
                        }, ensure_ascii=False)
                        
                        messages.append({
                            'role': 'tool',
                            'tool_call_id': tool_call.id,
                            'content': tool_response_content
                        })
                        
                        # æ›´æ–°è§†å›¾
                        if tool_result.get('success') and 'vega_spec' in tool_result:
                            current_spec = tool_result['vega_spec']
                            render_result = vega_service.render(current_spec)
                            
                            if render_result.get('success'):
                                current_image = render_result['image_base64']
                                print(f"    MCP è°ƒç”¨æˆåŠŸï¼Œè§†å›¾å·²æ›´æ–°")
                                
                                if GPT_CONFIG['save_images']:
                                    save_image(current_image, images_dir / f'iteration_{i+1}_{tool_name}.png')
                            else:
                                print(f"    æ¸²æŸ“å¤±è´¥")
                        elif tool_result.get('success'):
                            print(f"    MCP è°ƒç”¨æˆåŠŸï¼ˆåˆ†æå·¥å…·ï¼Œæ— è§†å›¾æ›´æ–°ï¼‰")
                        else:
                            print(f"    MCP è°ƒç”¨å¤±è´¥: {tool_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                            exploration['success'] = False
                else:
                    print("   ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
                
                # ========== é˜¶æ®µ2ï¼šåˆ†ææ€»ç»“ï¼ˆJSON è¾“å‡ºï¼‰ ==========
                print("\n é˜¶æ®µ2ï¼šåˆ†ææ€»ç»“...")
                
                messages.append(format_user_message_with_image(
                    get_analysis_prompt(),
                    current_image
                ))
                
                response2 = openai_client.chat.completions.create(
                    model=GPT_CONFIG['model'],
                    messages=messages,
                    temperature=GPT_CONFIG['temperature'],
                    response_format={"type": "json_object"}
                )
                
                message2 = response2.choices[0].message
                content2 = message2.content or ""
                
                print(f"\n GPT åˆ†æè¾“å‡º:")
                print("-" * 50)
                print(content2[:500] + "..." if len(content2) > 500 else content2)
                print("-" * 50)
                
                # è§£æ JSON
                parsed = parse_json_from_response(content2)
                
                if parsed:
                    key_insights = parsed.get('key_insights', [])
                    reasoning = parsed.get('reasoning', '')
                    exploration_complete = parsed.get('exploration_complete', False)
                    
                    exploration['analysis_summary']['key_insights'] = key_insights
                    exploration['analysis_summary']['reasoning'] = reasoning
                    
                    print(f"\n è§£æç»“æœ:")
                    print(f"  - key_insights: {len(key_insights)} æ¡")
                    print(f"  - exploration_complete: {exploration_complete}")
                else:
                    print(" æ— æ³•è§£æ JSON")
                    exploration_complete = False
                
                messages.append({'role': 'assistant', 'content': content2})
                explorations.append(exploration)
                
                if parsed and exploration_complete:
                    print(f"\n æ¢ç´¢å®Œæˆï¼Œå…± {i + 1} è½®")
                    break
    
    # 9. æ±‡æ€»æ‰€æœ‰æ´å¯Ÿ
    all_insights = []
    for exp in explorations:
        all_insights.extend(exp.get('analysis_summary', {}).get('key_insights', []))
    
    # 10. æ„å»ºæœ€ç»ˆç»“æœ
    result = {
        'task_id': task_id,
        'model': GPT_CONFIG['model'],
        'chart_type': chart_type,
        'query': query,
        'timestamp': datetime.now().isoformat(),
        'mode': 'gpt5_real_mcp_benchmark',
        'total_iterations': len(explorations),
        'explorations': explorations,
        'summary': {
            'all_insights': all_insights,
            'tools_called': all_tools_called
        }
    }
    
    # 11. ä¿å­˜ç»“æœ
    run_dir.mkdir(parents=True, exist_ok=True)
    result_path = run_dir / 'result.json'
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\n ç»“æœå·²ä¿å­˜: {result_path}")
    
    return result


def run_benchmark(task_path: str) -> dict:
    """è¿è¡Œ benchmark æµ‹è¯•ï¼ˆåŒæ­¥åŒ…è£…å™¨ï¼‰"""
    return asyncio.run(run_benchmark_async(task_path))


# =============================================================================
# å‘½ä»¤è¡Œå…¥å£
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='GPT Benchmark æµ‹è¯•')
    parser.add_argument('task_path', help='Benchmark ä»»åŠ¡ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', choices=['gpt-5', 'gpt-5', 'gpt-4o-mini'], default='gpt-5')
    parser.add_argument('--max-iterations', type=int, default=8)
    parser.add_argument('--base-url', default='https://api.oaipro.com/v1')
    parser.add_argument('--no-save-images', action='store_true')
    parser.add_argument('--max-tokens', type=int, default=2000)
    
    args = parser.parse_args()
    
    GPT_CONFIG['model'] = args.model
    GPT_CONFIG['max_iterations'] = args.max_iterations
    GPT_CONFIG['base_url'] = args.base_url
    GPT_CONFIG['save_images'] = not args.no_save_images
    
    print("=" * 70)
    print(" GPT Benchmark æµ‹è¯•")
    print("=" * 70)
    print(f" æ¨¡å‹: {GPT_CONFIG['model']}")
    print(f" ä»»åŠ¡: {args.task_path}")
    print(f" æœ€å¤§è½®æ•°: {GPT_CONFIG['max_iterations']}")
    print(f" MCP æœåŠ¡å™¨: {MCP_SERVER_PATH}")
    print("=" * 70)
    
    result = run_benchmark(args.task_path)
    
    if result:
        print("\n" + "=" * 70)
        print(" æµ‹è¯•å®Œæˆï¼")
        print("=" * 70)
        print(f" æ€»è½®æ•°: {result['total_iterations']}")
        print(f" å·¥å…·è°ƒç”¨: {result['summary']['tools_called']}")
        print(f" æ´å¯Ÿæ•°é‡: {len(result['summary']['all_insights'])}")
        
        if result['summary']['all_insights']:
            print(f"\n å‘ç°çš„æ´å¯Ÿ:")
            for idx, insight in enumerate(result['summary']['all_insights'][:5], 1):
                print(f"   {idx}. {insight}")
            if len(result['summary']['all_insights']) > 5:
                print(f"   ... è¿˜æœ‰ {len(result['summary']['all_insights']) - 5} æ¡")
        
        print("=" * 70)
    else:
        print("\n æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)


if __name__ == '__main__':
    main()

