"""
Subjective Answer Evaluator using LLM Judge

For key_insights:
- Groundedness (1-5): Coverage of GT content
- Depth (1-5): Specificity and data support
- Clarity (1-5): Clear and unambiguous

For reasoning (holistic, structured with tool info):
- Groundedness (1-5): Coverage of GT analytical steps
- Logical Coherence (1-5): Reasoning grounded in tool actions and context
- Clarity (1-5): Clear expression
"""

import json
import os
from typing import Dict, Any, List, Optional
from dataclasses import dataclass


@dataclass
class InsightEvalResult:
    groundedness: float     # 1-5
    depth: float            # 1-5
    clarity: float          # 1-5
    average_score: float    # Normalized to 0-1
    details: Dict[str, Any]


@dataclass
class ReasoningEvalResult:
    groundedness: float       # 1-5
    logical_coherence: float  # 1-5
    clarity: float            # 1-5
    average_score: float      # Normalized to 0-1
    details: Dict[str, Any]


@dataclass
class SubjectiveEvalResult:
    insight_score: float      # 0-1
    reasoning_score: float    # 0-1
    total_score: float        # 0-1
    insight_details: InsightEvalResult
    reasoning_details: ReasoningEvalResult


# ============================================================================
# Prompts
# ============================================================================

INSIGHT_EVAL_PROMPT = """You are an expert evaluator assessing the quality of insights generated by a visualization analysis agent.

## Ground Truth Insights (Reference):
{gt_insights}

## Agent's Key Insights:
{agent_insights}

## Evaluation Criteria:

**Groundedness (1-5):** Does the insight cover the content required by the GT?
- 1: Covers 0-20% of GT requirements
- 2: Covers 20-40%
- 3: Covers 40-60%
- 4: Covers 60-80%
- 5: Covers 80-100%

**Depth (1-5):** Is the insight specific with data support (not vague)?
- 1: Entirely vague
- 2: Superficial
- 3: Moderately detailed
- 4: Good with solid reasoning
- 5: Deep with precise data support

**Clarity (1-5):** Is the expression clear and unambiguous?
- 1: Confusing
- 3: Moderately clear
- 5: Crystal clear

## Output Format (JSON only):
{{
    "groundedness": <1-5>,
    "depth": <1-5>,
    "clarity": <1-5>,
    "explanation": "<brief explanation>"
}}"""


STRUCTURED_REASONING_EVAL_PROMPT = """You are evaluating the reasoning process of a visualization analysis agent.

## Ground Truth Reasoning Steps:
{gt_steps}

## Agent's Reasoning Steps:
{agent_steps}

## Evaluation Criteria:

**Groundedness (1-5):** Does the agent's reasoning cover the key analytical steps in the GT?
- Consider whether the agent identified similar analytical needs
- Tool choices don't need to match exactly but should serve similar analytical purposes
- 1: Missed all key steps; 3: Covered roughly half; 5: Covered all key steps

**Logical Coherence (1-5):** Is each step logically connected to the task goal?
- Are tool choices reasonable for the analytical task?
- Does the reasoning build progressively toward the answer?
- 1: Illogical; 3: Partially logical; 5: Fully logical

**Clarity (1-5):** Is the reasoning clearly expressed?
- 1: Very unclear; 3: Understandable; 5: Flows seamlessly

## Output Format (JSON only):
{{
    "groundedness": <1-5>,
    "logical_coherence": <1-5>,
    "clarity": <1-5>,
    "explanation": "<brief explanation>"
}}"""


class SubjectiveEvaluator:
    """Evaluator for subjective/open-ended questions using LLM Judge"""

    def __init__(self,
                 api_key_env: str = "OPENROUTER_API_KEY",
                 base_url: str = "https://openrouter.ai/api/v1",
                 model: Optional[str] = None,
                 timeout: int = 180):
        self.api_key_env = api_key_env
        self.base_url = base_url
        self.model = model or os.environ.get("OPENROUTER_EVAL_MODEL", "openai/gpt-5.2")
        self.timeout = timeout
        self._client = None

    def _get_client(self):
        if self._client is None:
            from openai import OpenAI
            api_key = os.environ.get(self.api_key_env)
            if not api_key:
                raise ValueError(f"Environment variable {self.api_key_env} not set")
            self._client = OpenAI(api_key=api_key, base_url=self.base_url, timeout=self.timeout)
        return self._client

    def evaluate(self,
                 agent_insights: List[str],
                 agent_reasoning_steps: List[Dict],
                 gt_insights: List[str],
                 gt_reasoning_steps: List[Dict],
                 is_fully_subjective: bool = True) -> SubjectiveEvalResult:
        """
        Evaluate agent's subjective output against ground truth.

        Args:
            agent_insights: Agent's key insights
            agent_reasoning_steps: [{iteration, tool, reasoning}, ...] — structured steps with tool info
            gt_insights: Ground truth insights
            gt_reasoning_steps: [{iteration, tool, reasoning}, ...] — GT structured steps
            is_fully_subjective: If True, groundedness gets full score for insights
        """
        insight_result = self._eval_insights(agent_insights, gt_insights, is_fully_subjective)
        reasoning_result = self._eval_reasoning(agent_reasoning_steps, gt_reasoning_steps, is_fully_subjective)

        total_score = 0.6 * insight_result.average_score + 0.4 * reasoning_result.average_score

        return SubjectiveEvalResult(
            insight_score=insight_result.average_score,
            reasoning_score=reasoning_result.average_score,
            total_score=total_score,
            insight_details=insight_result,
            reasoning_details=reasoning_result
        )

    # ==================== Insights ====================

    def _eval_insights(self, agent_insights: List[str], gt_insights: List[str],
                       is_fully_subjective: bool) -> InsightEvalResult:
        gt_text = "\n".join(f"- {ins}" for ins in gt_insights)
        agent_text = "\n".join(f"- {ins}" for ins in agent_insights)

        prompt = INSIGHT_EVAL_PROMPT.format(gt_insights=gt_text, agent_insights=agent_text)
        result = self._call_llm(prompt)

        groundedness = 5 if is_fully_subjective else result["groundedness"]
        depth = result["depth"]
        clarity = result["clarity"]
        average_score = (groundedness + depth + clarity) / 15.0

        return InsightEvalResult(
            groundedness=groundedness, depth=depth, clarity=clarity,
            average_score=average_score,
            details={"explanation": result.get("explanation", ""), "is_fully_subjective": is_fully_subjective}
        )

    # ==================== Reasoning (structured, holistic) ====================

    def _eval_reasoning(self, agent_steps: List[Dict], gt_steps: List[Dict],
                        is_fully_subjective: bool) -> ReasoningEvalResult:
        """
        Evaluate reasoning holistically: one LLM call for all steps.
        Both agent_steps and gt_steps are [{iteration, tool, reasoning}, ...].
        """
        if not agent_steps:
            return ReasoningEvalResult(
                groundedness=1, logical_coherence=1, clarity=1,
                average_score=0.2, details={"note": "no reasoning found"}
            )

        if not gt_steps:
            return ReasoningEvalResult(
                groundedness=5, logical_coherence=5, clarity=5,
                average_score=1.0, details={"note": "no GT reasoning, full score"}
            )

        gt_text = self._format_steps(gt_steps)
        agent_text = self._format_steps(agent_steps)

        prompt = STRUCTURED_REASONING_EVAL_PROMPT.format(gt_steps=gt_text, agent_steps=agent_text)
        result = self._call_llm(prompt)

        groundedness = 5 if is_fully_subjective else result["groundedness"]
        logical_coherence = result["logical_coherence"]
        clarity = result["clarity"]
        average_score = (groundedness + logical_coherence + clarity) / 15.0

        return ReasoningEvalResult(
            groundedness=groundedness, logical_coherence=logical_coherence, clarity=clarity,
            average_score=average_score,
            details={"explanation": result.get("explanation", ""), "agent_step_count": len(agent_steps), "gt_step_count": len(gt_steps)}
        )

    @staticmethod
    def _format_steps(steps: List[Dict]) -> str:
        """Format structured reasoning steps for prompt."""
        lines = []
        for s in steps:
            iteration = s.get("iteration", "?")
            tool = s.get("tool", "N/A")
            reasoning = s.get("reasoning", "")
            lines.append(f"Step {iteration} [tool: {tool}]: {reasoning}")
        return "\n".join(lines)

    # ==================== LLM Call ====================

    def _call_llm(self, prompt: str) -> Dict:
        """Call LLM and parse JSON response."""
        client = self._get_client()
        response = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0
        )

        response_text = response.choices[0].message.content.strip()

        # Strip markdown code fences if present
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]

        return json.loads(response_text)