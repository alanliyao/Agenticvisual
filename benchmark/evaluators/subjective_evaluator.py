"""
Subjective Answer Evaluator using LLM Judge

Evaluates open-ended answers using LLM as a judge:

For key_insights:
- Groundedness (1-5): Coverage of GT content
- Depth (1-5): Specificity and data support
- Clarity (1-5): Clear and unambiguous

For reasoning (per-round, averaged):
- Groundedness (1-5): Coverage of GT content
- Logical Coherence (1-5): Logical reasoning based on context
- Clarity (1-5): Clear expression

--------------------------------------------------------------------------------
Configuration (API key & model)

- API key: Set environment variable OPENROUTER_API_KEY.
  Configurable in SubjectiveEvaluator.__init__ via api_key_env (default
  "OPENROUTER_API_KEY").

- Model: SubjectiveEvaluator.__init__ argument `model`, default "gpt-4o".
  Override via env OPENROUTER_EVAL_MODEL if set.

- Base URL: __init__ argument `base_url`, default "https://openrouter.ai/api/v1".
"""

import json
import os
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass


@dataclass
class InsightEvalResult:
    """Insight evaluation result"""
    groundedness: float  # 1-5
    depth: float  # 1-5
    clarity: float  # 1-5
    average_score: float  # Normalized to 0-1
    details: Dict[str, Any]


@dataclass 
class ReasoningEvalResult:
    """Reasoning evaluation result"""
    groundedness: float  # 1-5
    logical_coherence: float  # 1-5
    clarity: float  # 1-5
    average_score: float  # Normalized to 0-1
    details: Dict[str, Any]


@dataclass
class SubjectiveEvalResult:
    """Combined subjective evaluation result"""
    insight_score: float  # 0-1
    reasoning_score: float  # 0-1
    total_score: float  # 0-1
    insight_details: InsightEvalResult
    reasoning_details: List[ReasoningEvalResult]


# ============================================================================
# Prompts
# ============================================================================

INSIGHT_EVAL_PROMPT = """You are an expert evaluator assessing the quality of insights generated by a visualization analysis agent.

## Ground Truth Insights (Reference):
{gt_insights}

## Agent's Key Insights:
{agent_insights}

## Evaluation Criteria:

**Groundedness (1-5):** Does the insight cover the content required by the GT?
- 1: Covers 0-20% of GT requirements
- 2: Covers 20-40% of GT requirements
- 3: Covers 40-60% of GT requirements
- 4: Covers 60-80% of GT requirements
- 5: Covers 80-100% of GT requirements

**Depth (1-5):** Is the insight specific with data support (not vague)?
- 1: Entirely vague; no specific data, values, or evidence cited
- 2: Superficial; rarely references concrete data
- 3: Moderately detailed; some analysis or explanation
- 4: Good explanation with solid reasoning, frequently cites specific values/regions/patterns
- 5: Deep explanation with examples and precise data support

**Clarity (1-5):** Is the expression clear and unambiguous?
- 1: Confusing and difficult to understand
- 2: Unclear; requires significant effort to interpret
- 3: Moderately clear; some ambiguity present
- 4: Clear and easy to understand; minor ambiguity
- 5: Crystal clear; precise and unambiguous throughout

## Output Format (JSON only):
{{
    "groundedness": <1-5>,
    "depth": <1-5>,
    "clarity": <1-5>,
    "explanation": "<brief explanation for scores>"
}}

Respond with valid JSON only."""


REASONING_EVAL_PROMPT = """You are an expert evaluator assessing the reasoning quality of a visualization analysis agent.

## Ground Truth Reasoning (Reference):
{gt_reasoning}

## Agent's Reasoning (for this iteration):
{agent_reasoning}

## Current View State:
{view_state}

## Evaluation Criteria:

**Groundedness (1-5):** Does the reasoning cover the GT requirements?
- 1: Covers 0-20% of GT requirements
- 2: Covers 20-40% of GT requirements
- 3: Covers 40-60% of GT requirements
- 4: Covers 60-80% of GT requirements
- 5: Covers 80-100% of GT requirements

**Logical Coherence (1-5):** Is the reasoning logical and grounded in context?
- 1: Illogical; disconnected from question, context, or visual content
- 2: Mostly illogical; rarely references actual view or context
- 3: Partially logical; some steps grounded in context but with gaps
- 4: Logical throughout; consistently references context with minor lapses
- 5: Fully logical; every step grounded in view state and context

**Clarity (1-5):** Is the expression clear and readable?
- 1: Very unclear; difficult to follow or disorganized
- 2: Somewhat unclear; main ideas partially obscured
- 3: Moderately clear; understandable with some effort
- 4: Clear, well-structured, and easy to read
- 5: Flows seamlessly with minimal effort to comprehend

## Output Format (JSON only):
{{
    "groundedness": <1-5>,
    "logical_coherence": <1-5>,
    "clarity": <1-5>,
    "explanation": "<brief explanation for scores>"
}}

Respond with valid JSON only."""


class SubjectiveEvaluator:
    """Evaluator for subjective/open-ended questions using LLM Judge"""
    
    def __init__(self, 
                 api_key_env: str = "OPENROUTER_API_KEY",
                 base_url: str = "https://openrouter.ai/api/v1",
                 model: Optional[str] = None,
                 timeout: int = 180):
        """
        Initialize LLM Judge evaluator.
        
        Args:
            api_key_env: Environment variable for API key (default OPENROUTER_API_KEY)
            base_url: API base URL (default OpenRouter)
            model: Model for evaluation; default "openai/gpt-5.2", or OPENROUTER_EVAL_MODEL env
            timeout: Request timeout
        """
        self.api_key_env = api_key_env
        self.base_url = base_url
        self.model = model or os.environ.get("OPENROUTER_EVAL_MODEL", "openai/gpt-5.2")
        self.timeout = timeout
        self._client = None
    
    def _get_client(self):
        """Get or create OpenAI client"""
        if self._client is None:
            from openai import OpenAI
            api_key = os.environ.get(self.api_key_env)
            if not api_key:
                raise ValueError(f"Environment variable {self.api_key_env} not set")
            self._client = OpenAI(
                api_key=api_key,
                base_url=self.base_url,
                timeout=self.timeout
            )
        return self._client
    
    def evaluate(self,
                 agent_insights: List[str],
                 agent_reasoning_rounds: List[Dict],
                 gt_insights: List[str],
                 gt_reasoning: str,
                 view_states: Optional[List[Dict]] = None,
                 is_fully_subjective: bool = True) -> SubjectiveEvalResult:
        """
        Evaluate agent's subjective output against ground truth.
        
        Args:
            agent_insights: Agent's key insights
            agent_reasoning_rounds: List of reasoning dicts per iteration
            gt_insights: Ground truth insights
            gt_reasoning: Ground truth reasoning
            view_states: List of view states per iteration (optional)
            is_fully_subjective: If True, groundedness gets full score
            
        Returns:
            SubjectiveEvalResult with all scores
        """
        # Evaluate insights
        insight_result = self._eval_insights(
            agent_insights, gt_insights, is_fully_subjective
        )
        
        # Evaluate reasoning (per-round, averaged)
        reasoning_results = self._eval_reasoning_rounds(
            agent_reasoning_rounds, gt_reasoning, view_states, is_fully_subjective
        )
        
        # Calculate average reasoning score
        if reasoning_results:
            reasoning_score = sum(r.average_score for r in reasoning_results) / len(reasoning_results)
        else:
            reasoning_score = 0.0
        
        # Calculate total score (weighted average)
        total_score = 0.6 * insight_result.average_score + 0.4 * reasoning_score
        
        return SubjectiveEvalResult(
            insight_score=insight_result.average_score,
            reasoning_score=reasoning_score,
            total_score=total_score,
            insight_details=insight_result,
            reasoning_details=reasoning_results
        )
    
    def _eval_insights(self, 
                       agent_insights: List[str],
                       gt_insights: List[str],
                       is_fully_subjective: bool = True) -> InsightEvalResult:
        """Evaluate insights using LLM Judge"""
        
        # Format insights for prompt
        gt_text = "\n".join([f"- {ins}" for ins in gt_insights])
        agent_text = "\n".join([f"- {ins}" for ins in agent_insights])
        
        prompt = INSIGHT_EVAL_PROMPT.format(
            gt_insights=gt_text,
            agent_insights=agent_text
        )
        
        try:
            result = self._call_llm(prompt)
            
            groundedness = result.get("groundedness", 3)
            depth = result.get("depth", 3)
            clarity = result.get("clarity", 3)
            
            # For fully subjective questions, groundedness is full score
            if is_fully_subjective:
                groundedness = 5
            
            # Normalize to 0-1
            average_score = (groundedness + depth + clarity) / 3 / 5
            
            return InsightEvalResult(
                groundedness=groundedness,
                depth=depth,
                clarity=clarity,
                average_score=average_score,
                details={
                    "explanation": result.get("explanation", ""),
                    "is_fully_subjective": is_fully_subjective
                }
            )
        except Exception as e:
            # Default scores on error
            return InsightEvalResult(
                groundedness=3,
                depth=3,
                clarity=3,
                average_score=0.6,
                details={"error": str(e)}
            )
    
    def _eval_reasoning_rounds(self,
                               reasoning_rounds: List[Dict],
                               gt_reasoning: str,
                               view_states: Optional[List[Dict]] = None,
                               is_fully_subjective: bool = True) -> List[ReasoningEvalResult]:
        """Evaluate each reasoning round"""
        results = []
        
        for i, round_data in enumerate(reasoning_rounds):
            reasoning_text = round_data.get("reasoning", "")
            view_state = view_states[i] if view_states and i < len(view_states) else {}
            
            result = self._eval_single_reasoning(
                reasoning_text, gt_reasoning, view_state, is_fully_subjective
            )
            results.append(result)
        
        return results
    
    def _eval_single_reasoning(self,
                               agent_reasoning: str,
                               gt_reasoning: str,
                               view_state: Dict,
                               is_fully_subjective: bool = True) -> ReasoningEvalResult:
        """Evaluate a single reasoning"""
        
        # Format view state for prompt
        view_state_text = json.dumps(view_state, ensure_ascii=False, indent=2) if view_state else "N/A"
        
        prompt = REASONING_EVAL_PROMPT.format(
            gt_reasoning=gt_reasoning,
            agent_reasoning=agent_reasoning,
            view_state=view_state_text[:2000]  # Truncate if too long
        )
        
        try:
            result = self._call_llm(prompt)
            
            groundedness = result.get("groundedness", 3)
            logical_coherence = result.get("logical_coherence", 3)
            clarity = result.get("clarity", 3)
            
            # For fully subjective questions, groundedness is full score
            if is_fully_subjective:
                groundedness = 5
            
            # Normalize to 0-1
            average_score = (groundedness + logical_coherence + clarity) / 3 / 5
            
            return ReasoningEvalResult(
                groundedness=groundedness,
                logical_coherence=logical_coherence,
                clarity=clarity,
                average_score=average_score,
                details={
                    "explanation": result.get("explanation", ""),
                    "is_fully_subjective": is_fully_subjective
                }
            )
        except Exception as e:
            # Default scores on error
            return ReasoningEvalResult(
                groundedness=3,
                logical_coherence=3,
                clarity=3,
                average_score=0.6,
                details={"error": str(e)}
            )
    
    def _call_llm(self, prompt: str) -> Dict:
        """Call LLM and parse JSON response"""
        client = self._get_client()
        
        response = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Parse JSON
        if response_text.startswith("```"):
            # Remove markdown code blocks
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
        
        return json.loads(response_text)


class MockSubjectiveEvaluator:
    """Mock evaluator for testing without API calls"""
    
    def evaluate(self,
                 agent_insights: List[str],
                 agent_reasoning_rounds: List[Dict],
                 gt_insights: List[str],
                 gt_reasoning: str,
                 view_states: Optional[List[Dict]] = None,
                 is_fully_subjective: bool = True) -> SubjectiveEvalResult:
        """Return mock evaluation results"""
        
        # Mock insight evaluation
        insight_result = InsightEvalResult(
            groundedness=5 if is_fully_subjective else 4,
            depth=4,
            clarity=4,
            average_score=0.8,
            details={"mock": True}
        )
        
        # Mock reasoning evaluation (one per round)
        reasoning_results = []
        for round_data in agent_reasoning_rounds:
            reasoning_results.append(ReasoningEvalResult(
                groundedness=5 if is_fully_subjective else 4,
                logical_coherence=4,
                clarity=4,
                average_score=0.8,
                details={"mock": True}
            ))
        
        reasoning_score = sum(r.average_score for r in reasoning_results) / len(reasoning_results) if reasoning_results else 0.0
        total_score = 0.6 * insight_result.average_score + 0.4 * reasoning_score
        
        return SubjectiveEvalResult(
            insight_score=insight_result.average_score,
            reasoning_score=reasoning_score,
            total_score=total_score,
            insight_details=insight_result,
            reasoning_details=reasoning_results
        )


# Convenience function
def evaluate_subjective(agent_insights: List[str],
                       agent_reasoning_rounds: List[Dict],
                       gt_insights: List[str],
                       gt_reasoning: str,
                       view_states: Optional[List[Dict]] = None,
                       is_fully_subjective: bool = True,
                       use_mock: bool = False) -> SubjectiveEvalResult:
    """Evaluate subjective answer"""
    if use_mock:
        evaluator = MockSubjectiveEvaluator()
    else:
        evaluator = SubjectiveEvaluator()
    return evaluator.evaluate(
        agent_insights, agent_reasoning_rounds, 
        gt_insights, gt_reasoning, 
        view_states, is_fully_subjective
    )
