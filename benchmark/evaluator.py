"""
Benchmarkè¯„ä¼°å™¨
è¯„ä¼°ç»´åº¦ï¼šæ´å¯Ÿè´¨é‡(60%)ã€æ¨ç†è¿‡ç¨‹(40%)
ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ´å¯ŸåŒ¹é…
"""

import json
from typing import Dict, List
import numpy as np
from sentence_transformers import SentenceTransformer


class BenchmarkEvaluator:
    """Benchmarkè¯„ä¼°å™¨
    
    è¯„ä¼°ç»´åº¦ï¼š
    1. æ´å¯Ÿè´¨é‡ (60%): Recall + Precision + Depth
    2. æ¨ç†è¿‡ç¨‹ (40%): è¿è´¯æ€§ + å·¥å…·è°ƒç”¨ + å·¥å…·è·¯å¾„ + æ¨ç†å¯¹é½
    """
    
    def __init__(self, ground_truth: Dict):
        self.gt = ground_truth
        print("ğŸ“¦ åŠ è½½è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹...")
        self.semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def evaluate(self, agent_result: Dict) -> Dict:
        """å®Œæ•´è¯„ä¼°"""
        explorations = agent_result.get('explorations', [])
        
        insight_score = self.evaluate_insight_quality(explorations)
        reasoning_score = self.evaluate_reasoning_process(explorations)
        
        weights = {'insight_quality': 0.60, 'reasoning_process': 0.40}
        total_score = insight_score * weights['insight_quality'] + reasoning_score * weights['reasoning_process']
        
        return {
            'total_score': round(total_score, 2),
            'dimension_scores': {
                'insight_quality': round(insight_score, 2),
                'reasoning_process': round(reasoning_score, 2)
            },
            'weights': weights,
            'details': {
                'total_explorations': len(explorations),
                'insights_found': self._count_insights(explorations),
                'tools_used': self._get_tools_used(explorations),
                'insights_before_dedup': getattr(self, '_dedup_stats', (0, 0))[0],
                'insights_after_dedup': getattr(self, '_dedup_stats', (0, 0))[1],
            }
        }
    
    def evaluate_insight_quality(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°æ´å¯Ÿè´¨é‡ï¼šRecall + Precision + Depth"""
        gt_insights = self.gt['insight_quality']['critical_insights']
        criteria = self.gt['insight_quality']['evaluation_criteria']
        
        # æ”¶é›†æ‰€æœ‰ key_insights
        agent_insights = []
        for exp in explorations:
            summary = exp.get('analysis_summary', {})
            agent_insights.extend(summary.get('key_insights', []))
        
        # è¯­ä¹‰å»é‡ï¼Œé™ä½é‡å¤æ´å¯Ÿå¯¹Precisionçš„ç¨€é‡Š
        before_cnt = len(agent_insights)
        agent_insights = self._dedup_insights(agent_insights)
        after_cnt = len(agent_insights)
        if before_cnt != after_cnt:
            print(f"ğŸ§¹ æ´å¯Ÿå»é‡: {before_cnt} -> {after_cnt}")
        self._dedup_stats = (before_cnt, after_cnt)
        
        if not agent_insights:
            return 0.0
        
        # Recall - æ¯ä¸ªGTæ´å¯Ÿçš„æœ€ä½³åŒ¹é…åˆ†æ•°
        recall_scores = [self._calc_match_score(gt, agent_insights) for gt in gt_insights]
        recall = np.mean(recall_scores)
        
        # Precision - agentæ´å¯Ÿä¸­æœ‰æ•ˆçš„æ¯”ä¾‹
        valid_count = sum(1 for ins in agent_insights if self._is_valid_insight(ins, gt_insights))
        precision = valid_count / len(agent_insights)
        
        # Depth - æ´å¯Ÿæ·±åº¦
        depth_scores = [self._assess_depth(ins) for ins in agent_insights]
        avg_depth = np.mean(depth_scores) / 3.0  # å½’ä¸€åŒ–åˆ°0-1
        
        score = (
            recall * criteria['recall_weight'] * 100 +
            precision * criteria['precision_weight'] * 100 +
            avg_depth * criteria['depth_weight'] * 100
        )
        
        return min(100, score)
    
    def evaluate_reasoning_process(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼šè¿è´¯æ€§ + å·¥å…·è°ƒç”¨ + å·¥å…·è·¯å¾„ + æ¨ç†å¯¹é½"""
        coherence = self._eval_coherence(explorations)
        tool_usage = self._eval_tool_usage(explorations)
        tool_path = self._eval_tool_path(explorations)
        reasoning_alignment = self._eval_reasoning_alignment(explorations)
        
        return (
            coherence * 0.15 +
            tool_usage * 0.35 +
            tool_path * 0.30 +
            reasoning_alignment * 0.20
        )
    
    # ========================================
    # æ´å¯Ÿè¯„ä¼°è¾…åŠ©æ–¹æ³•
    # ========================================
    
    def _calc_match_score(self, gt_insight: Dict, agent_insights: List[str]) -> float:
        """è®¡ç®—GTæ´å¯Ÿä¸agentæ´å¯Ÿçš„æœ€ä½³åŒ¹é…åˆ†æ•°ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        gt_content = gt_insight['content']
        gt_emb = self.semantic_model.encode(gt_content, convert_to_numpy=True)
        
        max_sim = 0.0
        for agent_ins in agent_insights:
            if not agent_ins or len(agent_ins.strip()) < 5:
                continue
            agent_emb = self.semantic_model.encode(agent_ins, convert_to_numpy=True)
            sim = np.dot(gt_emb, agent_emb) / (np.linalg.norm(gt_emb) * np.linalg.norm(agent_emb) + 1e-8)
            max_sim = max(max_sim, sim)
        
        return max_sim
    
    def _is_valid_insight(self, agent_insight: str, gt_insights: List[Dict]) -> bool:
        """æ£€æŸ¥agentæ´å¯Ÿæ˜¯å¦åŒ¹é…ä»»ä¸€GTæ´å¯Ÿï¼ˆé˜ˆå€¼0.5ï¼‰"""
        if not agent_insight or len(agent_insight.strip()) < 5:
            return False
        
        agent_emb = self.semantic_model.encode(agent_insight, convert_to_numpy=True)
        
        for gt in gt_insights:
            gt_emb = self.semantic_model.encode(gt['content'], convert_to_numpy=True)
            sim = np.dot(agent_emb, gt_emb) / (np.linalg.norm(agent_emb) * np.linalg.norm(gt_emb) + 1e-8)
            if sim > 0.5:
                return True
        
        return False
    
    def _assess_depth(self, insight: str) -> int:
        """è¯„ä¼°æ´å¯Ÿæ·±åº¦ï¼š1=æè¿°æ€§, 2=è¯Šæ–­æ€§, 3=é¢„æµ‹æ€§"""
        if not insight:
            return 1
        
        insight_lower = insight.lower()
        
        level3_kw = ['é¢„æµ‹', 'é¢„æœŸ', 'å°†ä¼š', 'ä¼šå¯¼è‡´', 'é¢„è®¡', 'will', 'forecast', 'predict', 'expect', 'å¦‚æœ', 'if']
        level2_kw = ['å› ä¸º', 'ç”±äº', 'å¯¼è‡´', 'åŸå› ', 'é€ æˆ', 'because', 'due to', 'caused by', 'æ‰€ä»¥', 'therefore', 'è¡¨æ˜']
        
        if any(kw in insight_lower for kw in level3_kw):
            return 3
        if any(kw in insight_lower for kw in level2_kw):
            return 2
        return 1

    def _dedup_insights(self, insights: List[str], threshold: float = 0.80) -> List[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å»é‡ï¼Œä¿ç•™ä»£è¡¨æ€§æ´å¯Ÿ"""
        deduped: List[str] = []
        if not insights:
            return deduped
        
        for ins in insights:
            if not ins or len(ins.strip()) < 5:
                continue
            ins_clean = ins.strip()
            ins_emb = self.semantic_model.encode(ins_clean, convert_to_numpy=True)
            
            is_dup = False
            for kept in deduped:
                kept_emb = self.semantic_model.encode(kept, convert_to_numpy=True)
                sim = float(np.dot(ins_emb, kept_emb) / (np.linalg.norm(ins_emb) * np.linalg.norm(kept_emb) + 1e-8))
                if sim >= threshold:
                    is_dup = True
                    break
            
            if not is_dup:
                deduped.append(ins_clean)
        
        return deduped
    
    # ========================================
    # æ¨ç†è¿‡ç¨‹è¯„ä¼°è¾…åŠ©æ–¹æ³•
    # ========================================
    
    def _eval_coherence(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°æ¨ç†è¿è´¯æ€§"""
        if len(explorations) <= 1:
            return 100.0
        
        score = 100.0
        
        for i in range(1, len(explorations)):
            prev = explorations[i-1]
            curr = explorations[i]
            
            # è·å–å‰ä¸€æ­¥æ´å¯Ÿ
            prev_insights = prev.get('analysis_summary', {}).get('key_insights', [])
            
            # è·å–å½“å‰æ­¥çš„ reasoning
            curr_reasoning = curr.get('analysis_summary', {}).get('reasoning', '')
            curr_tool = (curr.get('tool_execution') or {}).get('tool_name', '')
            
            # å¦‚æœå‰ä¸€æ­¥æœ‰æ´å¯Ÿï¼Œæ£€æŸ¥å½“å‰reasoningæ˜¯å¦æœ‰å¼•ç”¨
            if prev_insights and curr_reasoning:
                has_ref = any(self._concept_referenced(ins, curr_reasoning) for ins in prev_insights)
                if not has_ref:
                    score -= 5
            
            # æ£€æŸ¥é‡å¤å·¥å…·è°ƒç”¨
            if curr_tool == 'identify_clusters':
                used_before = any(
                    (exp.get('tool_execution') or {}).get('tool_name') == 'identify_clusters'
                    for exp in explorations[:i]
                )
                if used_before:
                    score -= 10
        
        return max(0, score)
    
    def _concept_referenced(self, insight: str, reasoning: str) -> bool:
        """æ£€æŸ¥æ´å¯Ÿæ¦‚å¿µæ˜¯å¦åœ¨reasoningä¸­è¢«å¼•ç”¨"""
        if not insight or not reasoning:
            return False
        
        insight_words = set(insight.lower().split())
        reasoning_lower = reasoning.lower()
        
        overlap = sum(1 for w in insight_words if w in reasoning_lower)
        return overlap / len(insight_words) > 0.3 if insight_words else False
    
    def _eval_tool_usage(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°å·¥å…·è°ƒç”¨è¦†ç›–ç‡"""
        if 'required_tools' not in self.gt.get('reasoning_process', {}):
            return 100.0
        
        required = set(self.gt['reasoning_process']['required_tools'])
        if not required:
            return 100.0
        
        used = set()
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            tool_name = tool_exec.get('tool_name')
            if tool_name:
                used.add(tool_name)
        
        coverage = len(required & used) / len(required)
        return coverage * 100
    
    def _eval_tool_path(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°å·¥å…·è°ƒç”¨è·¯å¾„ï¼ˆLCSç›¸ä¼¼åº¦ï¼‰"""
        if 'reference_optimal_path' not in self.gt.get('reasoning_process', {}):
            return 100.0
        
        gt_path = self.gt['reasoning_process']['reference_optimal_path']
        if not gt_path:
            return 100.0
        
        gt_seq = [step['tool'] for step in gt_path]
        
        agent_seq = []
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            tool_name = tool_exec.get('tool_name')
            if tool_name:
                agent_seq.append(tool_name)
        
        if not agent_seq:
            return 0.0
        
        lcs_len = self._lcs(gt_seq, agent_seq)
        return (lcs_len / len(gt_seq)) * 100

    def _eval_reasoning_alignment(self, explorations: List[Dict]) -> float:
        """è¯„ä¼°agentæ¨ç†ä¸GTå‚è€ƒæ¨ç†çš„å¯¹é½ç¨‹åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦å‡å€¼ï¼‰"""
        gt_reasonings = self._get_gt_reasonings()
        if not gt_reasonings:
            return 100.0
        
        agent_reasonings: List[str] = []
        for exp in explorations:
            reasoning = exp.get('analysis_summary', {}).get('reasoning', '')
            if isinstance(reasoning, list):
                reasoning_text = "\n".join(reasoning)
            else:
                reasoning_text = reasoning
            if reasoning_text and reasoning_text.strip():
                agent_reasonings.append(reasoning_text.strip())
        
        if not agent_reasonings:
            return 0.0
        
        gt_embs = [self.semantic_model.encode(r, convert_to_numpy=True) for r in gt_reasonings]
        
        scores = []
        for ar in agent_reasonings:
            ar_emb = self.semantic_model.encode(ar, convert_to_numpy=True)
            sims = [
                float(np.dot(ar_emb, gt_emb) / (np.linalg.norm(ar_emb) * np.linalg.norm(gt_emb) + 1e-8))
                for gt_emb in gt_embs
            ]
            scores.append(max(sims) if sims else 0.0)
        
        return min(100.0, float(np.mean(scores) * 100))

    def _get_gt_reasonings(self) -> List[str]:
        """æå–GTå‚è€ƒæ¨ç†æ–‡æœ¬"""
        reasoning_process = self.gt.get('reasoning_process', {})
        ref_path = reasoning_process.get('reference_optimal_path', []) or []
        texts = []
        for step in ref_path:
            text = step.get('reasoning')
            if text and isinstance(text, str) and text.strip():
                texts.append(text.strip())
        return texts
    
    def _lcs(self, seq1: List[str], seq2: List[str]) -> int:
        """æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    # ========================================
    # ç»Ÿè®¡æ–¹æ³•
    # ========================================
    
    def _count_insights(self, explorations: List[Dict]) -> int:
        """ç»Ÿè®¡æ´å¯Ÿæ•°é‡"""
        count = 0
        for exp in explorations:
            count += len(exp.get('analysis_summary', {}).get('key_insights', []))
        return count
    
    def _get_tools_used(self, explorations: List[Dict]) -> List[str]:
        """è·å–ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨"""
        tools = []
        for exp in explorations:
            tool_exec = exp.get('tool_execution') or {}
            tool_name = tool_exec.get('tool_name')
            if tool_name:
                tools.append(tool_name)
        return tools


def format_evaluation_report(eval_result: Dict, task_id: str) -> str:
    """æ ¼å¼åŒ–è¯„ä¼°æŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append(f"Benchmarkè¯„ä¼°æŠ¥å‘Š - {task_id}")
    report.append("=" * 60)
    report.append("")
    
    report.append(f"ğŸ“Š æ€»åˆ†: {eval_result['total_score']}/100")
    report.append("")
    
    scores = eval_result['dimension_scores']
    weights = eval_result['weights']
    
    report.append("ğŸ“ˆ å„ç»´åº¦å¾—åˆ†:")
    report.append(f"  1. æ´å¯Ÿè´¨é‡ ({int(weights['insight_quality']*100)}%): {scores['insight_quality']}/100")
    report.append(f"  2. æ¨ç†è¿‡ç¨‹ ({int(weights['reasoning_process']*100)}%): {scores['reasoning_process']}/100")
    report.append("")
    
    details = eval_result['details']
    report.append("ğŸ“‹ æ¢ç´¢è¯¦æƒ…:")
    report.append(f"  - æ¢ç´¢è½®æ¬¡: {details['total_explorations']}")
    report.append(f"  - å‘ç°æ´å¯Ÿ: {details['insights_found']}ä¸ª")
    report.append(f"  - ä½¿ç”¨å·¥å…·: {', '.join(details['tools_used']) if details['tools_used'] else 'æ— '}")
    report.append("")
    
    total = eval_result['total_score']
    if total >= 85:
        rating = "ğŸŒŸ ä¼˜ç§€"
    elif total >= 70:
        rating = "âœ… è‰¯å¥½"
    elif total >= 60:
        rating = "âš ï¸ åŠæ ¼"
    else:
        rating = "âŒ ä¸åŠæ ¼"
    
    report.append(f"è¯„çº§: {rating}")
    report.append("=" * 60)
    
    return "\n".join(report)


if __name__ == "__main__":
    print("Benchmarkè¯„ä¼°å™¨å·²å°±ç»ª")
    print("æ”¯æŒå­—æ®µ: explorations[].analysis_summary.key_insights")
    print("æ”¯æŒå­—æ®µ: explorations[].analysis_summary.reasoning")
    print("æ”¯æŒå­—æ®µ: explorations[].tool_execution.tool_name")